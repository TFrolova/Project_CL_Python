# Project_CL_Python. Частоты слов в "Маленьком принце"
Проект представляет собой сравнение результатов разных подсчетов частот в "Маленьком принце".
Такая работа делалась и делается постоянно, даже есть онлайн сервисы, которые по заданному тексту изготавливают облако слов.

Главной целью проекта было понять, чем отличаются разные способы подсчета и их результаты, предположить, какой способ актуален для какой задачи.

1) Данные для исследования

В качестве материала для исследования взяты тексты корпуса СинТагРус вообще и входящий в этот корпус размеченный текст сказки А. де Сент-Экзюпери "Маленький принц" в переводе Н. Галь, длина этого текста около полутора тысяч предложений.
СинТагРус содержит около полутора миллионов слововхождений в 1304 текстах. Поиск по нему доступен на сайте ruscorpora.ru.

Разметка СинТагРуса хранится в xml-формате.

К СинТагРусу нет свободного доступа, поэтому в папке data внутри проекта нет файлов корпуса, там хранятся списки лемм и отношений, извлеченные из текстов корпуса.

Из-за специфики данных, задачи токенизации и лемматизации были неактуальны (хотя по касательной пришлось иметь дело с токенизацией в связи с применением библиотеки sklearn)

2) Инструменты исследования и вспомогательные средства

-  Notepad++

- библиотеки Питона

- материалы занятий

3) Результаты 

Сравнивались три способа сопоставления частот слов внутри текста: простой подсчет абсолютных частот, подсчет относительных частот (не делалось умножения на миллион, но по сути это как ipm), tfidf, характеристика weirdness в исходном и немного модифицированном виде.

- Результат простого подсчета абсолоютных частот может понадобится в ситуации, когда нужно составить словарь для обработки данного текста в исследовательских или учебных челях.

- Вне сравнения с данными других текстов относительные частоты не очень отличаются в возможностях применения от абсолютных. Однако относительные частоты в разных текстах можно сравнивать и делать выводы как о поведении отдельных слов, так и о характеристиках текста в сравнении с корпусом.

У первых двух способов есть особенность - невозможно различить слова характерные и нехарактерные для данного текста. Именно поэтому при таких подсчетах часто испольуют списки стоп-слов.

- Характеристика weirdness определяется в литературе как результат деления относительной частоты слова в тексте на относительную частоту слова в корпусе. Если корпус недостаточно велик, то эта характеристика выдвигает на первый план слова, которые случайно оказались уникальными для данного текста. Оказалось, что СинТагРус как раз недостаточно велик для корпуса с таким разнообразным по темам и жанрам содержанием. Поэтому в нашем тексте при таком подсчете на первый план вышли не ожидаемые слова "маленький", "принц", "лис", а "флюгер", "заглатывать", "по-турецки", "понемножку" и т.д. Это связано с тем, что для слов, которые встречаются только в рассматриваемом тексте и больше нигде в корпусе, характеристика weirdness никак не зависит от того, сколько раз это слово встретилось. Частота слов попадает и в числитель и в знаменатель и сокращается, мы видим в этом случае фактически только соотношение размеров корпусов, а характеристика weirdness для слова "фонарщик", которое встретилось в тексте 22 раза и кажется довольно важным для сказки, совпадает с этой характеристикой для слова "понемножку", которое встретилось в тексте всего один раз, но больше в СинТагРусе не встречалось. Можно себе представить, что список такого рода понадобится для каких-то задач, но это явно не самые очевидные задачи.

- Модифицированная характеристика weirdness. Для того, чтобы поправить свойство неучета количества слов, уникальных для текста, попробовала возвести в квадрат относительную частоту слова в тексте, то есть числитель. Результат получился гораздо ближе к интуитивному представлению. На мой вкус вообще самый приятный результат из всех полученных, если цель получить что-то вроде лексического портрета текста. Подсчет абсолютной, относительной частот, показателя weirdness и weirdness с числителем в квадрате находится в этой тетради: https://github.com/TFrolova/Project_CL_Python/blob/main/notebooks/Counter%20Frequencies%20Weirdness.ipynb.

- Подсчет по характеристике tfidf. Сделан при помощи библиотеке sklearn. Тоже получился некоторый неплохой результат. Подробнее в этой тетради: https://github.com/TFrolova/Project_CL_Python/blob/main/notebooks/TfIdf%20sklearn%20for%20%22Маленький%20принц%22%20-%20LEMMAS.ipynb.

4) внутри папки проекта содержатся папки 
- data (списки отношений и лемм, а также игрушечный файл из двух коротких предложений с разметкой как в СинТагРусе), 
- notebooks
 тетрадки с кодом для извлечения данных из xml документа СинТагРуса, 
 для получения абсолютных и относительных частот лемм, 
 для получения списков лемм, упорядоченных по tfifd)
 
Вторая и третья тетрадки дублированы для отношений. Казалось неясным, можно ли получить какой-то внятный результат от подсчета отношений. Не удалось, то есть я не смогла увидеть ничего или почти путного в результатах подсчета.
 
 - pictures с картинками, которые использовались для получения облака слов в условно симпатичном виде.
 
 - preliminary exercise - папка с подготовительным упражнением по xml - в тетрадке из этой папке код для разделения размеченного файла "Маленького принца" на много размеченных файлов, содержащих одгно предложение, это не имеет непосредственного отношения к цели проекта, но так как тренировка и улучшение навыков обращения с xml разметкой была одним из важных для автора побочных результатов, оставляю эту папку и тетрадку внутри нее здесь.
